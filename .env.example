# =============================================================================
# ARCHIVO DE CONFIGURACIÓN DE EJEMPLO - .env.example
# =============================================================================
# Copie este archivo a .env y complete los valores antes de ejecutar.
# NUNCA suba el archivo .env con valores reales al repositorio.
# =============================================================================

# -----------------------------------------------------------------------------
# Configuración de la API
# -----------------------------------------------------------------------------
# Clave secreta para autenticación (CAMBIE ESTE VALOR EN PRODUCCIÓN)
API_KEY=su_api_key_secreta_aqui

# Puerto donde escuchará la API
API_PORT=8000

# Host de la API (0.0.0.0 para aceptar conexiones de cualquier origen)
API_HOST=0.0.0.0

# -----------------------------------------------------------------------------
# Configuración de Ollama (Servidor de modelos de IA)
# -----------------------------------------------------------------------------
# Hostname del servidor Ollama (usar 'ollama' si está en Docker Compose)
OLLAMA_HOST=ollama

# Puerto del servidor Ollama (puerto por defecto es 11434)
OLLAMA_PORT=11434

# Nombre del modelo a utilizar
# qwen2.5:1.5b - Más rápido, menor precisión (recomendado para clasificación)
# qwen2.5:3b - Balance velocidad/precisión
# qwen2.5:7b - Mayor precisión, más lento
MODEL_NAME=qwen2.5:3b

# -----------------------------------------------------------------------------
# Configuración de Recursos (para Docker)
# -----------------------------------------------------------------------------
# Número de hilos que Ollama puede usar para procesamiento
OLLAMA_NUM_THREADS=8

# Número máximo de modelos cargados simultáneamente en memoria
OLLAMA_MAX_LOADED_MODELS=1

# -----------------------------------------------------------------------------
# Optimizaciones de Velocidad y PARALELISMO
# -----------------------------------------------------------------------------
# Tiempo para mantener modelos en memoria (5m, 10m, 15m, 30m)
OLLAMA_KEEP_ALIVE=30m

# Número de solicitudes procesables en paralelo (ajustar según RAM)
# Con 8, puedes hacer hasta 8 llamadas simultáneas a Ollama
OLLAMA_NUM_PARALLEL=8

# Flash Attention: mejora rendimiento en múltiples requests (1=activado)
OLLAMA_FLASH_ATTENTION=1

# Distribuir carga entre núcleos (1=activado)
OLLAMA_SCHED_SPREAD=1
