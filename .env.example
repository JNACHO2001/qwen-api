# =============================================================================
# ARCHIVO DE CONFIGURACIÓN DE EJEMPLO - .env.example
# =============================================================================
# Copie este archivo a .env y complete los valores antes de ejecutar.
# NUNCA suba el archivo .env con valores reales al repositorio.
# =============================================================================

# -----------------------------------------------------------------------------
# Configuración de la API
# -----------------------------------------------------------------------------
# Clave secreta para autenticación (CAMBIE ESTE VALOR EN PRODUCCIÓN)
API_KEY=su_api_key_secreta_aqui

# Puerto donde escuchará la API
API_PORT=8000

# Host de la API (0.0.0.0 para aceptar conexiones de cualquier origen)
API_HOST=0.0.0.0

# -----------------------------------------------------------------------------
# Configuración de Ollama (Servidor de modelos de IA)
# -----------------------------------------------------------------------------
# Hostname del servidor Ollama (usar 'ollama' si está en Docker Compose)
OLLAMA_HOST=ollama

# Puerto del servidor Ollama (puerto por defecto es 11434)
OLLAMA_PORT=11434

# Nombre del modelo a utilizar
# Opciones comunes: qwen2.5:3b, qwen2.5:7b, llama3:8b, mistral:7b
MODEL_NAME=qwen2.5:3b

# -----------------------------------------------------------------------------
# Configuración de Recursos (para Docker)
# -----------------------------------------------------------------------------
# Número de hilos que Ollama puede usar para procesamiento
OLLAMA_NUM_THREADS=8

# Número máximo de modelos cargados simultáneamente en memoria
OLLAMA_MAX_LOADED_MODELS=1
